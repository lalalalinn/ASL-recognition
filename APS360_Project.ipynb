{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2DGtqHmUBZF"
      },
      "source": [
        "# Importing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwTyu9mjcCpt"
      },
      "outputs": [],
      "source": [
        "# To Download the WLASL (World Level American Sign Language) Video Kaggle Databse\n",
        "# https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtXB4qrjjaxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f30d6c-9a9d-4f23-fac0-2e667293f28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7c4onA5UJD3"
      },
      "outputs": [],
      "source": [
        "# data_path = '/content/drive/MyDrive/aps360 labs/' # NOTE: May be person-dependent depending on google drive structure\n",
        "data_path = '/content/drive/MyDrive/aps360 labs/APS360'\n",
        "json_path = data_path+\"/WLASL_v0.3.json\"\n",
        "with open(json_path, 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl4rPDq1ULzn"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EArdZDWvVhgY"
      },
      "source": [
        "Convert from videos to images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL8LYIEPVz_F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "def vid_to_img(vid_file, img_dir, frame_interval=1):\n",
        "  \"\"\" From a video url, generate a series of frames (images) with a certain frame frequency.\n",
        "      The images are stored automatically to the directory specified.\n",
        "\n",
        "    Args:\n",
        "        vid_file: The URL of the video file we want to convert.\n",
        "        img_dir: The path directory for where the images will be stored.\n",
        "        frame_interval: The frame frequency in which the frames will be taken.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #open video file\n",
        "  capture = cv2.VideoCapture(vid_file)\n",
        "  num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  if capture.isOpened() == False or num_frames == 0:\n",
        "    print(\"Error opening video stream or file\")\n",
        "    return False\n",
        "\n",
        "  #create img_dir if not exit\n",
        "  os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "  max_frames = 15\n",
        "  frame_count = 0\n",
        "  saved_frame = 0\n",
        "  success = True\n",
        "\n",
        "  # Loop trough all frames in the video\n",
        "  while saved_frame < max_frames:\n",
        "    success, frame = capture.read()\n",
        "    if not success:\n",
        "      break\n",
        "\n",
        "    # frame saved every frame_interval number of frame\n",
        "    if frame_count % frame_interval == 0 and frame_count >20:\n",
        "      frame_path = os.path.join(img_dir, f\"frame_{saved_frame +1}.jpg\")\n",
        "      cv2.imwrite(frame_path, frame)\n",
        "      saved_frame += 1\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "  #clear space\n",
        "  capture.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  return True\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/APS360-data'\n",
        "\n",
        "# make the directories if they do not exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "os.makedirs(os.path.join(save_path, 'Train-48'), exist_ok = True)\n",
        "os.makedirs(os.path.join(save_path, 'Val-48'), exist_ok=True)\n",
        "os.makedirs(os.path.join(save_path, 'Test-48'),exist_ok = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "gHgXzZBS3LDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset seems to be already split into train, val and test for us\n",
        "#Only run this once\n",
        "num_processed = 0\n",
        "num_train = 0\n",
        "num_val =0\n",
        "num_test = 0\n",
        "\n",
        "for video_class in data:\n",
        "  label = video_class['gloss']\n",
        "\n",
        "  for instance in video_class['instances']:\n",
        "    if num_processed % 50 == 0:\n",
        "       print(num_processed)\n",
        "\n",
        "    id = instance ['video_id']\n",
        "    vid_file = instance['url']\n",
        "\n",
        "    if instance['split'] == 'train':\n",
        "      if(num_train >2000):\n",
        "        break\n",
        "      train_dir = os.path.join(save_path, 'Train-48',label,id)\n",
        "      if vid_to_img(vid_file, train_dir) == True:\n",
        "        num_train += 1\n",
        "        num_processed += 1\n",
        "\n",
        "\n",
        "    elif instance['split'] == 'val':\n",
        "      if(num_val >1000):\n",
        "        break\n",
        "      val_dir = os.path.join(save_path, 'Val-48', label,id)\n",
        "      if vid_to_img(vid_file, val_dir) == True:\n",
        "          num_val += 1\n",
        "          num_processed += 1\n",
        "\n",
        "    else:\n",
        "      if(num_test >500):\n",
        "        break\n",
        "      test_dir = os.path.join(save_path, 'Test-48', label,id)\n",
        "      if vid_to_img(vid_file, test_dir) == True:\n",
        "          num_test += 1\n",
        "          num_processed += 1\n",
        "\n",
        "print(\"total processed: \", num_processed)\n"
      ],
      "metadata": {
        "id": "OREMmZmq3hex",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "d8df6be2-479c-4cf5-fa4a-5dff94829226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9d7d0ddb5396>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train-48'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mvid_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mnum_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnum_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e593d103ca71>\u001b[0m in \u001b[0;36mvid_to_img\u001b[0;34m(vid_file, img_dir, frame_interval)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#open video file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mcapture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7xdUnwYeNFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "vid_path = '/content/drive/MyDrive/aps360 labs/Project'\n",
        "save_path = '/content/drive/MyDrive/aps360 labs/APS360/Train/frames'\n",
        "extra_labels = [\"help\", \"who\", \"what\", \"want\", \"computer\", \"family\", \"hot\",\"yes\", \"time\", \"clothes\"]\n",
        "num = 100000\n",
        "for label in extra_labels:\n",
        "\n",
        "  label_dir = os.path.join(vid_path, label)\n",
        "  id_sequence = np.array([id for id in os.listdir(label_dir)] )\n",
        "\n",
        "  for id in id_sequence:\n",
        "\n",
        "    vid_dir = os.path.join(label_dir, id)\n",
        "    save_dir = os.path.join(save_path,label,str(num))\n",
        "    vid_to_img(vid_dir, save_dir,2)\n",
        "    num+=1"
      ],
      "metadata": {
        "id": "kRzafrClx1SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "Ogf8ypMWR6i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qml_pDnUSNzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ASLDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, videos, labels):\n",
        "    self.videos = videos\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.videos[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "ajynarQNqlO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\"Train\", \"Val\", \"Test\"]\n",
        "\n",
        "dataset_ASL = []\n",
        "dataset_loaders = []\n",
        "\n",
        "\n",
        "# For Data Augmentation\n",
        "transform = v2.Compose(\n",
        "[\n",
        " v2.RandomHorizontalFlip(p=0.5), # horizontal flip\n",
        " v2.Resize(size=(255, 255)), # resize resolution\n",
        " v2.ColorJitter(brightness = 0.3, saturation=0.5), # +-30% change in brightness, +-50% change in saturation,\n",
        " v2.RandomRotation([-30,+30]), # random rotate +- 30 degrees\n",
        " v2.GaussianBlur(31)]\n",
        ")\n",
        "\n",
        "class_cnt = 0\n",
        "class_map = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "  dataset_path = os.path.join(data_path, dataset)\n",
        "\n",
        "  dataset_video_list = [] # full dataset as a list, each being (video, label)\n",
        "  dataset_label_list = []\n",
        "\n",
        "\n",
        "\n",
        "  for class_name in sorted(os.listdir(dataset_path)): # apple, africa, chair, ...\n",
        "    class_path = os.path.join(dataset_path, class_name)\n",
        "\n",
        "    # Get the mapped class map\n",
        "    if class_name not in class_map:\n",
        "      class_map[class_name] = class_cnt\n",
        "      class_cnt += 1\n",
        "\n",
        "    # tensor representing the \"label\" for all the videos in this class\n",
        "    class_tensor = torch.tensor([class_map[class_name]])\n",
        "\n",
        "\n",
        "\n",
        "    for video_id in os.listdir(class_path):\n",
        "      video_path = os.path.join(class_path, video_id)\n",
        "\n",
        "      frames = os.listdir(video_path) # List of frames for that video\n",
        "      # Remove any non .jpg files\n",
        "      frames = [x for x in frames if (len(x)<4 or x[-3]!=\".jpg\")]\n",
        "\n",
        "      # tensor to hold all the frames for this video\n",
        "      video_tensor = []\n",
        "\n",
        "      # Must keep track of this so the SAME transform is applied to ALL frames in the video\n",
        "      first_transform = False\n",
        "      transform_state = None\n",
        "\n",
        "      for frame_name in frames:\n",
        "        frame_path = os.path.join(video_path, frame_name)\n",
        "        frame_np = np.array(plt.imread(frame_path)) # read image/frame\n",
        "        # apply data augmentation\n",
        "\n",
        "        if first_transform == False:\n",
        "          # first transform on the video, do it normally\n",
        "          transform(frame_np) # This is needed to generate a new rng_state, different than the previous video\n",
        "\n",
        "          transform_state = torch.get_rng_state()\n",
        "          frame_np = transform(frame_np)\n",
        "\n",
        "          first_transform = True\n",
        "        else:\n",
        "          # load same transform state\n",
        "          torch.set_rng_state(transform_state)\n",
        "          frame_np = transform(frame_np)\n",
        "\n",
        "        # make copy of numpy version to prevent unwritable tensor conversion\n",
        "        frame_tensor = torch.from_numpy(np.copy(frame_np))\n",
        "\n",
        "        # append this image to the tensor for the entire video\n",
        "        video_tensor.append(frame_tensor)\n",
        "\n",
        "      video_tensor = torch.from_numpy(np.asarray(video_tensor)) # Convert to tensor\n",
        "\n",
        "    dataset_video_list.append(video_tensor)\n",
        "    dataset_label_list.append(class_tensor.detach().clone())\n",
        "\n",
        "  # create dataset\n",
        "  dataset_video_list = np.asarray(dataset_video_list)\n",
        "  dataset_label_list = np.asarray(dataset_label_list)\n",
        "\n",
        "  dataset_ASL.append(ASLDataset(dataset_video_list, dataset_label_list))\n",
        "  dataset_loaders.append(torch.utils.data.DataLoader(dataset_ASL[-1], batch_size=1))\n"
      ],
      "metadata": {
        "id": "QQ7svRl4R8dJ",
        "outputId": "8c7b0221-849b-4e27-eecb-b7a1c21bd886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'v2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b754d0f85c8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m transform = v2.Compose(\n\u001b[0m\u001b[1;32m      8\u001b[0m  [torchvision.transforms.ToTensor(),\n\u001b[1;32m      9\u001b[0m   \u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# horizontal flip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'v2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model\n",
        "Using 3d convolutional networks\n"
      ],
      "metadata": {
        "id": "pICzDeqrfoDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "class cnn3d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cnn3d, self).__init__()\n",
        "        self.name = \"CNN3\"\n",
        "        self.conv1 = nn.Conv3d(48,64,(3,3,3),(1,2,2),(1,1,1))\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.conv2 = nn.Conv3d(64,128,(3,3,3),(1,2,2),(1,1,1))\n",
        "        self.conv3 = nn.Conv3d(128,256,(3,3,3),(1,2,2),(1,1,1))\n",
        "        self.fc1 = nn.Linear(256 * 3 * 4 * 4,1024)\n",
        "        self.fc2 = nn.Linear(1024,44)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
        "        x = torch.flatten(x,1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eHzFxaKHgEmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class getDataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None, num_frames=15): # Add num_frames parameter\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.classes = os.listdir(root_dir)\n",
        "    self.files = [] #Holds tuples of path to video folder and class label\n",
        "    self.num_frames = num_frames # Store the desired number of frames\n",
        "\n",
        "    for index, label in enumerate(self.classes):\n",
        "      class_path = os.path.join(root_dir, label)\n",
        "      for video_folder in os.listdir(class_path):\n",
        "        #Ignore checkpoints folder don't know why it's accessing it\n",
        "        if video_folder == '.ipynb_checkpoints':\n",
        "          continue\n",
        "        video_path = os.path.join(class_path, video_folder)\n",
        "        self.files.append((video_path, index)) #store the index of class for memroy-efficiency and avoid the need to convert later\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "          video_path, label = self.files[idx]\n",
        "          frame_files = [f for f in sorted(os.listdir(video_path)) if f.endswith('.jpg')and not os.path.isdir(os.path.join(video_path, f))]\n",
        "          images = []\n",
        "          # Sample a fixed number of frames, ensuring not to exceed available frames\n",
        "          num_frames_to_sample = min(self.num_frames, len(frame_files))\n",
        "          selected_frames = np.linspace(0, len(frame_files) - 1, num_frames_to_sample, dtype=int)\n",
        "          for i in selected_frames:\n",
        "              frame_file = frame_files[i]\n",
        "              frame_path = os.path.join(video_path, frame_file)\n",
        "              try:\n",
        "                  image = Image.open(frame_path).convert('RGB')\n",
        "                  if self.transform:\n",
        "                      image = self.transform(image)\n",
        "                  images.append(image)\n",
        "              except Exception as e:\n",
        "                  print(f\"Error loading image {frame_path}: {e}\")\n",
        "\n",
        "          # Pad the images list if it has fewer frames than num_frames\n",
        "          # Handle the case when images is empty\n",
        "          if images: # Check if images list is not empty\n",
        "            while len(images) < self.num_frames:\n",
        "                images.append(torch.zeros_like(images[0]))  # Assuming images[0] exists\n",
        "          else:\n",
        "            # Handle the case when no images were loaded (e.g., return an empty tensor or raise an error)\n",
        "            return torch.zeros((self.num_frames, 3, 240, 240)), label # Return a tensor of zeros with the expected shape\n",
        "\n",
        "          if len(images) == 0:\n",
        "              raise ValueError(f\"No images could be loaded for video {video_path}\")\n",
        "\n",
        "          images = torch.stack(images, dim=0)\n",
        "          return images, label\n"
      ],
      "metadata": {
        "id": "p8_f6FUvfyRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curve(path):\n",
        "  \"\"\" Plots the training curve for a model run, given the csv files\n",
        "  containing the train/validation error/loss.\n",
        "  Args:\n",
        "  path: The base path of the csv files produced during training\n",
        "  \"\"\"\n",
        "  import matplotlib.pyplot as plt\n",
        "  train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "  #val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "  plt.title(\"Validation Error\")\n",
        "  n = len(train_err) # number of epochs\n",
        "  plt.plot(range(1,n+1), train_err, label=\"Validation\")\n",
        " # plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "q6t9eXNhpKl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline(model, num_epochs=20, lr = 0.001, batch_size = 64):\n",
        "  torch.manual_seed(1000)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  transform = transforms.Compose([transforms.Resize((240,240), interpolation=transforms.InterpolationMode.BILINEAR),transforms.ToTensor()])\n",
        "\n",
        "  #get dataset\n",
        "  # root_dir = r'C:\\Users\\Sihan Chen\\Downloads\\APS360'\n",
        "  # train_dir = os.path.join(root_dir, 'small_train')\n",
        "  # print(train_dir)\n",
        "  # train_dataset = getDataset(root_dir=train_dir,transform=transform,num_frames=15)\n",
        "  # train_dataset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  # data, label = train_dataset[0]  # Attempt to retrieve the first item\n",
        "  # print(\"Sample loaded successfully:\", data.shape, label)\n",
        "  # if torch.cuda.is_available():\n",
        "    # model = model.cuda()\n",
        "\n",
        "  #define overall loss and accuracy\n",
        "  total_train_acc = np.zeros(num_epochs)\n",
        "  total_train_loss = np.zeros(num_epochs)\n",
        "  total_val_acc = np.zeros(num_epochs)\n",
        "  total_val_loss = np.zeros(num_epochs)\n",
        "\n",
        "  #train\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train() #training mode enables dropout and batch notmalization\n",
        "    print(\"in training\")\n",
        "    train_loss = 0.0\n",
        "    train_corr = 0\n",
        "    total_train = 0\n",
        "    for inputs, labels in dataloader:\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs) #forward pass\n",
        "      loss = criterion(outputs, labels) #calculate loss\n",
        "      loss.backward() #backward pass\n",
        "      optimizer.step() #update weights\n",
        "\n",
        "      train_loss += loss.item() * inputs.size(0)\n",
        "      total_train += labels.size(0)\n",
        "      train_corr += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_acc = train_corr /total_train\n",
        "    total_train_acc[epoch] = train_acc\n",
        "    total_train_loss[epoch] = train_loss / total_train\n",
        "\n",
        "\n",
        "    val_dir = os.path.join(root_dir, 'small_val')\n",
        "    val_dataset = getDataset(root_dir=val_dir,transform=transform)\n",
        "    val_dataset_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corr = 0\n",
        "    total_val = 0\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():  # This ensures no gradients are computed during validation\n",
        "        for v_inputs, v_labels in val_dataloader:\n",
        "            v_inputs = v_inputs.to(device)  # Move inputs to the correct device\n",
        "            v_labels = v_labels.to(device)  # Move labels to the correct device\n",
        "\n",
        "            v_outputs = model(v_inputs)\n",
        "            v_loss = criterion(v_outputs, v_labels)\n",
        "            val_loss += v_loss.item() * v_inputs.size(0)\n",
        "            val_corr += (v_outputs.argmax(1) == v_labels).sum().item()\n",
        "            total_val += v_labels.size(0)\n",
        "\n",
        "        val_acc = val_corr / total_val\n",
        "        total_val_acc[epoch]= val_acc\n",
        "        total_val_loss[epoch] = val_loss / total_val\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}:')\n",
        "    print(f'Train Loss: {train_loss / total_train:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "    print(f'Validation Loss: {val_loss / total_val:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "  path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,batch_size,lr,num_epochs+1)\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "  np.savetxt(\"{}_train_acc.csv\".format(path), total_train_acc)\n",
        "  np.savetxt(\"{}_train_loss.csv\".format(path), total_train_loss)\n",
        "  np.savetxt(\"{}_val_err.csv\".format(path), total_val_acc)\n",
        "  np.savetxt(\"{}_val_loss.csv\".format(path), total_val_loss)\n",
        "\n",
        "  plot_training_curve(path)\n"
      ],
      "metadata": {
        "id": "KqMpboFRgUod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline =cnn3d()\n",
        "train_baseline(baseline, num_epochs=10, lr = 0.001, batch_size = 64)\n"
      ],
      "metadata": {
        "id": "wo-gMpfvtT7p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "fabf1a2d-e11f-4a65-9574-7c188a1f148e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample loaded successfully: torch.Size([48, 3, 240, 240]) 0\n",
            "in training\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ef8ff1a208e4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcnn3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-32180a79013d>\u001b[0m in \u001b[0;36mtrain_baseline\u001b[0;34m(model, num_epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-71ed58e5c8f7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0mframe_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZjnEpUxEz96"
      },
      "source": [
        "# Build and Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvmje2cADZs"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python mediapipe scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bAr0NXe7Z-W"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.0 opencv-python mediapipe scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "e1ltnd0Eet4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Run mediapipe holistic to extract keypoints\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "co_Itclre2ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise mediapipe components\n",
        "mp_holistic = mp.solutions.holistic\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "metadata": {
        "id": "V6zZU5yvfGVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_with_mediapipe(frame, model):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_rgb.flags.writeable = False\n",
        "    detections = model.process(frame_rgb)\n",
        "    frame_rgb.flags.writeable = True\n",
        "    frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
        "    return frame_bgr, detections"
      ],
      "metadata": {
        "id": "7JOI9a90fIU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_landmarks(frame, detections):\n",
        "    # Draw facial landmarks\n",
        "    mp_drawing.draw_landmarks(frame, detections.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                                    mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                                    mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                                    )\n",
        "    mp_drawing.draw_landmarks(frame, detections.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                                    mp_drawing.DrawingSpec(color=(255, 200, 200), thickness=1, circle_radius=1),\n",
        "                                    mp_drawing.DrawingSpec(color=(200, 255, 200), thickness=1, circle_radius=1)\n",
        "                                    )\n",
        "    # Draw pose landmarks\n",
        "    mp_drawing.draw_landmarks(frame, detections.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                                    mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                                    mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                                    )\n",
        "    # Draw left hand landmarks\n",
        "    mp_drawing.draw_landmarks(frame, detections.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                                    mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                                    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                                    )\n",
        "    # Draw right hand landmarks\n",
        "    mp_drawing.draw_landmarks(frame, detections.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                                    )"
      ],
      "metadata": {
        "id": "3Swq1hPzfLUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(detections):\n",
        "    pose = np.array([[detection.x, detection.y, detection.z, detection.visibility] for detection in detections.pose_landmarks.landmark]).flatten() if detections.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[detection.x, detection.y, detection.z] for detection in detections.face_landmarks.landmark]).flatten() if detections.face_landmarks else np.zeros(468*3)\n",
        "    lefth = np.array([[detection.x, detection.y, detection.z] for detection in detections.left_hand_landmarks.landmark]).flatten() if detections.left_hand_landmarks else np.zeros(21*3)\n",
        "    righth = np.array([[detection.x, detection.y, detection.z]for detection in detections.right_hand_landmarks.landmark]).flatten() if detections.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lefth, righth])"
      ],
      "metadata": {
        "id": "4bO0pmujfM3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for storing data\n",
        "DATA_DIRECTORY = '/content/drive/MyDrive/aps360 labs/APS360/test/frames'#'/content/drive/MyDrive/aps360 labs/APS360/Val'\n",
        "# TRAIN_DATA_DIRECTORY = '/content/drive/MyDrive/aps360 labs/APS360/Train/frames'\n",
        "\n",
        "# Obtain labels\n",
        "gesture_labels = np.array([folder for folder in os.listdir(DATA_DIRECTORY) if os.path.isdir(os.path.join(DATA_DIRECTORY,folder))] )\n",
        "\n",
        "gesture_labels= np.sort(gesture_labels)\n",
        "\n",
        "small_labels = gesture_labels[:10]\n",
        "\n",
        "frame_count = 15\n",
        "\n",
        "print(small_labels)\n",
        "\n",
        "# Words with manual videos\n",
        "extra_labels = [\"help\", \"want\", \"family\",\"yes\", \"time\",\"clothes\"]\n"
      ],
      "metadata": {
        "id": "szyKW8zOfO8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN THIS ONCE\n",
        "# capture = cv2.VideoCapture(0)\n",
        "# Initialize MediaPipe holistic model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic_model:\n",
        "\n",
        "    # NEW LOOP\n",
        "    # Iterate through gestures\n",
        "    for gesture in extra_labels:\n",
        "        # Iterate through video sequences\n",
        "        vid_dir=os.path.join(DATA_DIRECTORY,gesture)\n",
        "\n",
        "        id_sequence = np.array([id for id in os.listdir(vid_dir) if os.path.isdir(os.path.join(vid_dir,id))] )\n",
        "\n",
        "\n",
        "        for sequence_id in id_sequence:\n",
        "            # Iterate through frames in each video sequence\n",
        "            for frame_number in range(frame_count):\n",
        "\n",
        "                frame_dir = os.path.join(vid_dir,sequence_id,str(gesture)+\"_\"+str(frame_number)+'.jpg')\n",
        "                # Capture frame from video\n",
        "                frame = cv2.imread(frame_dir)\n",
        "\n",
        "                if(frame is None):\n",
        "                  print(sequence_id)\n",
        "                  break\n",
        "\n",
        "\n",
        "\n",
        "                # Process frame with MediaPipe\n",
        "                frame_bgr, detections = process_image_with_mediapipe(frame, holistic_model)\n",
        "\n",
        "                # Render landmarks on frame\n",
        "                render_landmarks(frame_bgr, detections)\n",
        "\n",
        "                # NEW Apply waiting logic\n",
        "                # if frame_number == 0:\n",
        "                #     cv2.putText(frame_bgr, 'STARTING COLLECTION', (120,200),\n",
        "                #                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "                #     cv2.putText(frame_bgr, f'Collecting frames for {gesture} Video Number {sequence_id}', (15,12),\n",
        "                #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                #     # Display frame\n",
        "                #     cv2.imshow('OpenCV Feed', frame_bgr)\n",
        "                #     cv2.waitKey(500)\n",
        "                # else:\n",
        "                #     cv2.putText(frame_bgr, f'Collecting frames for {gesture} Video Number {sequence_id}', (15,12),\n",
        "                #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                #     # Display frame\n",
        "                #     cv2.imshow('OpenCV Feed', frame_bgr)\n",
        "\n",
        "                # NEW Export keypoints\n",
        "                keypoints = extract_keypoints(detections)\n",
        "\n",
        "                npy_file_path = os.path.join(DATA_DIRECTORY, gesture, str(sequence_id), str(frame_number))\n",
        "                frame_path = os.path.join(DATA_DIRECTORY, gesture, str(sequence_id), str(frame_number)+'.npy')\n",
        "\n",
        "                np.save(npy_file_path, keypoints)\n",
        "\n",
        "                # Exit gracefully\n",
        "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "    # capture.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "z3a1OmWXfQ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "gesture_to_index = {gesture:num for num, gesture in enumerate(extra_labels)}\n",
        "gesture_to_index"
      ],
      "metadata": {
        "id": "Uf8lxiW7fRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_sequences, target_labels = [], []\n",
        "val_sequences, val_labels =[],[]\n",
        "test_sequences, test_labels = [],[]\n",
        "train_dir = '/content/drive/MyDrive/aps360 labs/APS360/Train/frames'\n",
        "val_dir = '/content/drive/MyDrive/aps360 labs/APS360/Val'\n",
        "test_dir = '/content/drive/MyDrive/aps360 labs/APS360/test/frames'\n",
        "for gesture in extra_labels:\n",
        "    for sequence_id in np.array(os.listdir(os.path.join(train_dir, gesture))):\n",
        "        frame_data = []\n",
        "        frames = np.array(os.listdir(os.path.join(train_dir, gesture, sequence_id)))\n",
        "        frames = sorted(frames,key=natural_keys)\n",
        "\n",
        "        for i in range(15) :\n",
        "            frame_number = frames[i]\n",
        "            result = np.load(os.path.join(train_dir, gesture, str(sequence_id), f\"{frame_number}\"))\n",
        "            frame_data.append(result)\n",
        "        frame_date = np.array(frame_data)\n",
        "        data_sequences.append(frame_data)\n",
        "        target_labels.append(gesture_to_index[gesture])\n",
        "\n",
        "    for vid_id in np.array(os.listdir(os.path.join(val_dir, gesture))):\n",
        "        val_frame = []\n",
        "        for frame_number in range(frame_count):\n",
        "            val_result = np.load(os.path.join(val_dir, gesture, str(vid_id), f\"{frame_number}.npy\"))\n",
        "            if val_result is None:\n",
        "                val_result = np.load(os.path.join(val_dir, gesture, str(vid_id), \"ezgif-frame-00\"+str(frame_number)+\".npy\"))\n",
        "            val_frame.append(val_result)\n",
        "        val_sequences.append(val_frame)\n",
        "        val_labels.append(gesture_to_index[gesture])\n",
        "\n",
        "    for test_id in np.array(os.listdir(os.path.join(test_dir, gesture))):\n",
        "        test_frame = []\n",
        "        for frame_number in range(frame_count):\n",
        "            test_result = np.load(os.path.join(test_dir, gesture, str(test_id), f\"{frame_number}.npy\"))\n",
        "            test_frame.append(test_result)\n",
        "        test_sequences.append(test_frame)\n",
        "        test_labels.append(gesture_to_index[gesture])"
      ],
      "metadata": {
        "id": "36089XpUfVcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================== Prepare train/val/test dataset\n",
        "\n",
        "\n",
        "\n",
        "x_train = np.array(data_sequences)\n",
        "x_val = np.array(val_sequences)\n",
        "x_test = np.array(test_sequences)\n",
        "\n",
        "num_classes = len(extra_labels)\n",
        "# Convert numerical labels back to gesture names\n",
        "gesture_names = [extra_labels[i] for i in target_labels]\n",
        "val_gestures = [extra_labels[i] for i in val_labels]\n",
        "test_gestures = [extra_labels[i] for i in test_labels]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_indices = torch.tensor([gesture_to_index[gesture] for gesture in gesture_names])  # Use gesture names to get indices\n",
        "val_indices = torch.tensor([gesture_to_index[gesture] for gesture in val_gestures])\n",
        "test_indices = torch.tensor([gesture_to_index[gesture]for gesture in test_gestures])\n",
        "\n",
        "y_train = F.one_hot(label_indices, num_classes).int()\n",
        "y_val = F.one_hot(val_indices, num_classes).int()\n",
        "y_test = F.one_hot(test_indices, num_classes).int()\n",
        "\n",
        "# ============= Creating pytorch dataset and loader\n",
        "y_train = y_train.numpy()\n",
        "labels_l = []\n",
        "for i in range(len(y_train)):\n",
        "  labels_l.append(y_train[i])\n",
        "\n",
        "for i in range(len(data_sequences)):\n",
        "  data_sequences[i] = np.array(data_sequences[i])\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(labels_l, dtype=torch.float32)\n",
        "\n",
        "print(isinstance(x_train, torch.Tensor))\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "# ============== Create validation datasete and loader\n",
        "y_val = y_val.numpy()\n",
        "labels_v = []\n",
        "for i in range(len(y_val)):\n",
        "  labels_v.append(y_val[i])\n",
        "\n",
        "for i in range(len(val_sequences)):\n",
        "  val_sequences[i] = np.array(val_sequences[i])\n",
        "\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(labels_v, dtype=torch.float32)\n",
        "print(len(x_val))\n",
        "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ============== Create test datasete and loader\n",
        "y_test = y_test.numpy()\n",
        "labels_t = []\n",
        "for i in range(len(y_test)):\n",
        "  labels_t.append(y_test[i])\n",
        "\n",
        "for i in range(len(test_sequences)):\n",
        "  test_sequences[i] = np.array(test_sequences[i])\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(labels_t, dtype=torch.float32)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ==============Finish\n",
        "\n",
        "\n",
        "x_train =torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "test_indices\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.05)"
      ],
      "metadata": {
        "id": "BUvgw9nwfXZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xeqg39m5fiVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVp_T8eMBiLn"
      },
      "source": [
        "Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mY19q1No7shQ"
      },
      "outputs": [],
      "source": [
        "# Define LSTM model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        self.lstm_layer1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm_layer2 = nn.LSTM(hidden_dim, 128, batch_first=True)\n",
        "        self.lstm_layer3 = nn.LSTM(128, hidden_dim, batch_first=True)\n",
        "        self.fc_layer1 = nn.Linear(hidden_dim, 18)\n",
        "        # self.fc_layer2 = nn.Linear(64, 32)\n",
        "        self.fc_layer3 = nn.Linear(18, num_classes)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output_layer = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm_layer1(x)\n",
        "        x = self.dropout(x)\n",
        "        # x = self.activation(x)\n",
        "        # x, _ = self.lstm_layer2(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.activation(x)\n",
        "        # x, _ = self.lstm_layer3(x)\n",
        "        # x = self.activation(x)\n",
        "        x = self.fc_layer1(x[:, -1, :])\n",
        "        x = self.activation(x)\n",
        "        # x = self.fc_layer2(x)\n",
        "        # x = self.activation(x)\n",
        "        x = self.fc_layer3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiLKecfPBvPz"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u41TkUoD7v-0"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "input_dim = 1662\n",
        "hidden_dim = 256\n",
        "\n",
        "# Initialize model\n",
        "model = LSTMNetwork(input_dim, hidden_dim,num_classes)\n",
        "print(num_classes)\n",
        "torch.manual_seed(1000)\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.0009)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 700\n",
        "\n",
        "train_acc_plot, train_loss_plot, val_acc_plot, val_loss_plot = [],[],[],[]\n",
        "val_top2_plot = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_corr = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predictions = model(inputs)\n",
        "      # print(predictions.shape)\n",
        "      loss = loss_fn(predictions, torch.argmax(labels, dim=1))\n",
        "\n",
        "      # loss = loss_fn(predictions, y_train)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute accuracy\n",
        "      # print(labels.size(0))\n",
        "      threshold = 0.5\n",
        "      pred_classes = (predictions >= threshold).float()\n",
        "      pred_labels = predictions.argmax(dim=1)\n",
        "      true_labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "      train_loss += loss.item() * inputs.size(0)\n",
        "      total_train += labels.size(0)\n",
        "      train_corr += (pred_labels == true_labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    train_acc = train_corr / total_train\n",
        "    train_loss = train_loss / total_train\n",
        "\n",
        "    train_acc_plot.append(train_acc)\n",
        "    train_loss_plot.append(train_loss)\n",
        "\n",
        "\n",
        "      # _, predicted_labels = torch.max(predictions.data, 1)\n",
        "      # _, true_labels = torch.max(y_train, 1)  # Convert one-hot encoded labels to class indices\n",
        "      # accuracy = (predicted_labels == true_labels).sum().item() / y_train.size(0)\n",
        "\n",
        "    # Compute Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corr = 0\n",
        "    total_val = 0\n",
        "    top2_corr = 0.0\n",
        "    stop = False\n",
        "    with torch.no_grad():  # This ensures no gradients are computed during validation\n",
        "        for v_inputs, v_labels in val_dataloader:\n",
        "\n",
        "            v_outputs = model(v_inputs)\n",
        "\n",
        "            val_pred = (v_outputs >= threshold).float()\n",
        "            val_pred_labels = v_outputs.argmax(dim=1)\n",
        "            val_true_labels = torch.argmax(v_labels, dim=1)\n",
        "\n",
        "            v_labels = v_labels.argmax(dim=1)\n",
        "            top2_preds =v_outputs.topk(2, dim=1).indices\n",
        "\n",
        "            top2_labels = v_labels.unsqueeze(1).expand_as(top2_preds)\n",
        "\n",
        "            correct = (top2_preds == top2_labels).any(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "            v_loss = loss_fn(v_outputs, v_labels)\n",
        "            val_loss += v_loss.item() * v_inputs.size(0)\n",
        "            val_corr += (val_pred_labels == val_true_labels).sum().item()\n",
        "            total_val += v_labels.size(0)\n",
        "            top2_corr += correct.sum().item()\n",
        "\n",
        "        val_acc = val_corr / total_val\n",
        "        val_loss = val_loss / total_val\n",
        "\n",
        "        val_acc_plot.append(val_acc)\n",
        "        val_loss_plot.append(val_loss)\n",
        "        val_top2 = top2_corr/ total_val\n",
        "        val_top2_plot.append(val_top2)\n",
        "    # Print every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs},Train Loss: {loss.item()},Train Accuracy: {train_acc * 100:.2f}%')\n",
        "        print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc * 100:.2f}%, top2 val acc: {val_top2 * 100:.2f}%')\n",
        "        if (val_acc > 0.9):\n",
        "          if(stop):\n",
        "            break\n",
        "          stop = True\n",
        "# tensorboard_writer.close()\n",
        "\n",
        "# Model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N63ioGCX7y15"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the results\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "n = 470 # number of epochs\n",
        "plt.plot(range(1,n + 1), train_loss_plot, label=\"Train\")\n",
        "plt.plot(range(1,n + 1), val_loss_plot, label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Train vs Validation Accuracy \")\n",
        "plt.plot(range(1,n + 1), train_acc_plot, label=\"Train\")\n",
        "plt.plot(range(1,n + 1), val_acc_plot, label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Validation Top 2 Accuracy \")\n",
        "plt.plot(range(1,n + 1),val_top2_plot, label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Top-2 Accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXBcjsE_E5Hn"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "\n",
        "test_loss = 0.0\n",
        "test_corr = 0\n",
        "total_test = 0\n",
        "top2_corr = 0.0\n",
        "test_acc_plot, test_loss_plot = [],[]\n",
        "with torch.no_grad():  # This ensures no gradients are computed during validation\n",
        "        for t_inputs, t_labels in test_dataloader:\n",
        "\n",
        "            t_outputs = model(t_inputs)\n",
        "\n",
        "\n",
        "            test_pred_labels = t_outputs.argmax(dim=1)\n",
        "            test_true_labels = torch.argmax(t_labels, dim=1)\n",
        "\n",
        "            t_labels = t_labels.argmax(dim=1)\n",
        "            top2_preds =t_outputs.topk(2, dim=1).indices\n",
        "\n",
        "            top2_labels = t_labels.unsqueeze(1).expand_as(top2_preds)\n",
        "\n",
        "            correct = (top2_preds == top2_labels).any(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "            t_loss = loss_fn(t_outputs, t_labels)\n",
        "            test_loss += t_loss.item() * t_inputs.size(0)\n",
        "            test_corr += (test_pred_labels == test_true_labels).sum().item()\n",
        "            total_test += t_labels.size(0)\n",
        "            top2_corr += correct\n",
        "\n",
        "        test_acc = test_corr / total_test\n",
        "        test_loss = test_loss / total_test\n",
        "\n",
        "test_top2 = top2_corr.sum().item() / total_test\n",
        "print(f'Test Accuracy: {test_acc * 100: .2f}%, Test Loss: {test_loss},Top 2 accuracy: {test_top2 *100 :.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mpjswaCfLIB",
        "outputId": "cb04b0e4-9f7b-47f1-bbcb-846bc0e88634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.7.4)\n",
            "Downloading gTTS-2.5.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torchmetrics\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Predicting the outputs\n",
        "    test_predictions = model(x_test)\n",
        "\n",
        "    # Get the predicted class labels\n",
        "    _, predicted_labels = torch.max(test_predictions, 1)\n",
        "\n",
        "    # Convert y_test to tensor\n",
        "    true_labels = torch.argmax(y_test, dim=1)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy_metric = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
        "accuracy = accuracy_metric(predicted_labels, true_labels)\n",
        "print(f'Accuracy: {accuracy.item()}')\n",
        "\n",
        "# Convert tensors to numpy arrays for scikit-learn functions\n",
        "true_labels_np = true_labels.cpu().numpy()\n",
        "predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "\n",
        "# Calculate multilabel confusion matrix\n",
        "confusion_matrix = confusion_matrix(true_labels_np, predicted_labels_np)\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "pG8FwNqJfbdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_accuracies = confusion_matrix.diagonal() / confusion_matrix.sum(axis=1)\n",
        "\n",
        "for i, class_accuracy in enumerate(class_accuracies):\n",
        "    print(f'Accuracy for class {i}: {class_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "KP5HdEw1f6jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demonstartion and Evaluation on New Data"
      ],
      "metadata": {
        "id": "FpeUfJuPvKN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyttsx3"
      ],
      "metadata": {
        "id": "oeurQQmNgMzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyttsx3"
      ],
      "metadata": {
        "id": "P0_Ahi7lgNoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [(255, 0, 0),(0, 255, 0),(0, 0, 255),(255, 255, 0), (128, 0, 128),(0, 128, 128) ]\n",
        "#(255, 0, 0),(0, 255, 0),(0, 0, 255),(255, 255, 0), (0, 255, 255), (255, 0, 255),(128, 128, 0),(128, 0, 128),(0, 128, 128),(128, 128, 128)\n",
        "#Blue, Green, Red, Cyan, Yellow, Magenta, Olive, Purple, Teal, Grey\n",
        "def probability_visual(result, gestures, in_frame, colors):\n",
        "    out_frame = in_frame.copy()\n",
        "    for num, prob in enumerate(result):\n",
        "        cv2.rectangle(out_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
        "        cv2.putText(out_frame, extra_labels[num], (0, 85+num*40), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 2, cv2.LINE_AA)\n",
        "\n",
        "    return out_frame"
      ],
      "metadata": {
        "id": "IbZLQfvsfev8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = pyttsx3.init()"
      ],
      "metadata": {
        "id": "TqLfIZnovJAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#can run this code only on jupyter notebook as it tries to access the webcam\n",
        "import torch\n",
        "\n",
        "sqn = []\n",
        "phrase = []\n",
        "guess = []\n",
        "probChosen = 0.9\n",
        "\n",
        "extra_labels = np.array([\"help\", \"want\", 'family', 'yes', 'time', 'clothes'])\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "        det, frame = cap.read()\n",
        "\n",
        "        framebgr, detection = process_image_with_mediapipe(frame, holistic)\n",
        "        print(detection)\n",
        "\n",
        "        render_landmarks(framebgr, detection)\n",
        "\n",
        "        keypoints = extract_keypoints(detection)\n",
        "        sqn.append(keypoints)\n",
        "        sqn = sqn[-20:]\n",
        "\n",
        "        if len(sqn) == 20:\n",
        "            # Convert sqn to a PyTorch tensor\n",
        "            input_tensor = torch.tensor(np.expand_dims(sqn, axis=0)).float()\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Make predictions\n",
        "            with torch.no_grad():\n",
        "                res = model(input_tensor)[0]\n",
        "            predicted_label = torch.argmax(res).item()\n",
        "            print(extra_labels[predicted_label])\n",
        "            guess.append(predicted_label)\n",
        "\n",
        "            if np.unique(guess[-10:])[0] == predicted_label:\n",
        "                if res[predicted_label].item() > probChosen:\n",
        "\n",
        "                    if len(phrase) > 0:\n",
        "                        if extra_labels[predicted_label] != phrase[-1]:\n",
        "                            phrase.append(extra_labels[predicted_label])\n",
        "                            engine.say(extra_labels[np.argmax(res)])\n",
        "                            engine.runAndWait()\n",
        "                    else:\n",
        "                        phrase.append(extra_labels[predicted_label])\n",
        "                        engine.say(extra_labels[np.argmax(res)])\n",
        "                        engine.runAndWait()\n",
        "\n",
        "            if len(phrase) > 5:\n",
        "                phrase = phrase[-5:]\n",
        "\n",
        "            framebgr = probability_visual(res.cpu().numpy(), extra_labels, framebgr, colors)\n",
        "\n",
        "        cv2.rectangle(framebgr, (0,430), (640, 500), (0, 0, 0), -1)\n",
        "        cv2.putText(framebgr, ' '.join(phrase), (2,460),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.imshow('render', framebgr)\n",
        "\n",
        "        if cv2.waitKey(10) & 0xFF == ord('v'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "jVvcoMFxvf0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}